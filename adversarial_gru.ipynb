{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.load( 'training_data.npy')\n",
    "y=np.load( 'training_labels.npy')\n",
    "test_data_x=np.load( 'test_data.npy')\n",
    "test_labels_y=np.load( 'test_labels.npy')\n",
    "class_labels= np.array([0]*82858+[1]*7354+[2]*480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(class_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_data_x), torch.from_numpy(test_labels_y))\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, input_dim, hidden_dim, n_layers, drop_prob=0.15):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.GRU(input_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        hidden=self.init_hidden(batch_size)\n",
    "        #hidden = tuple([each.data for each in hidden])\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator (nn.Module ):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self ).__init__()\n",
    "\n",
    "        outdim = 2440\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.leakyReLU = nn.LeakyReLU()\n",
    "        self.conv_1 = nn.Conv1d(1, 3, kernel_size=27, stride=1)\n",
    "        self.conv_2 = nn.Conv1d(3, 10, kernel_size=14, stride=1)\n",
    "        self.conv_3 = nn.Conv1d(10, 10, kernel_size=3, stride=1)\n",
    "        self.conv_4 = nn.Conv1d(10, 10, kernel_size=4, stride=1)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(outdim,30)\n",
    "        self.fc2 = nn.Linear(30,10)\n",
    "        self.fc3 = nn.Linear(10,3)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_4.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x ):\n",
    "        # 1\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_1(x ) ) )\n",
    "        # 3\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_2(x ) ) )\n",
    "        # 5\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_3(x ) ) )\n",
    "        # 7\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_4(x ) ) )\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 9        \n",
    "        x = self.leakyReLU(self.fc1(x ) )\n",
    "        # 10\n",
    "        x = self.leakyReLU(self.fc2(x ) )\n",
    "    \n",
    "        x = self.fc3(x )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (leakyReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (conv_1): Conv1d(1, 3, kernel_size=(27,), stride=(1,))\n",
      "  (conv_2): Conv1d(3, 10, kernel_size=(14,), stride=(1,))\n",
      "  (conv_3): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
      "  (conv_4): Conv1d(10, 10, kernel_size=(4,), stride=(1,))\n",
      "  (fc1): Linear(in_features=2440, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "# Create the Discriminator\n",
    "cnn = Discriminator().to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    cnn = nn.DataParallel(cnn, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "cnn.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentGRU(\n",
       "  (lstm): GRU(1, 256, num_layers=2, dropout=0.15)\n",
       "  (dropout): Dropout(p=0.15)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "output_size = 1\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentGRU(output_size, input_dim, hidden_dim, n_layers)\n",
    "#net.load_state_dict(torch.load(\"lstm_adam\",map_location=\"cuda:2\"))\n",
    "#net.load_state_dict(torch.load(\"lstm10\",map_location=\"cpu\"))\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "net.to(device)\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-20\n",
    "def accuracy(outputs, labels):\n",
    "    temp=(outputs==labels)\n",
    "    return temp.sum()/ (labels.shape[0])\n",
    "\n",
    "def true_positive(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==2).sum()\n",
    "    \n",
    "def true_negative(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==0).sum()\n",
    "\n",
    "def false_positive(outputs, labels):\n",
    "    temp= labels-outputs\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def false_negative(outputs, labels):\n",
    "    temp=outputs-labels\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_positive(outputs, labels))\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_negative(outputs, labels))\n",
    "\n",
    "def BCR(outputs, labels):\n",
    "    return (precision(outputs, labels)+recall(outputs, labels))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_seq, sequence_length):\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    #feature_tensor = torch.from_numpy(test_seq)\n",
    "    feature_tensor=test_seq.view(4000,sequence_length, 1)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    \n",
    "    # get the output from the model\n",
    "    feature_tensor = feature_tensor.type(torch.FloatTensor)\n",
    "    feature_tensor = feature_tensor.to(device)\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "   # print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "  #  if(pred.item()==1):\n",
    "  #      print(\"AFIB wave\")\n",
    " #   else:\n",
    "   #     print(\"Normal wave\")\n",
    "    \n",
    "    return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.6929347826086957.....Precision=0.6133315540134784.....Recall=0.8557065723328989....F1 score=0.714524466555249\n",
      "Epoch: 1/50... Step: 1418... Tr Loss: -0.262314... Discr Loss: 0.869013...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eerun/Python-3.7.2/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.6402591973244147.....Precision=0.5789318165029179.....Recall=0.7295661887916589....F1 score=0.645578483463075\n",
      "Epoch: 2/50... Step: 2836... Tr Loss: -0.408242... Discr Loss: 0.863819...\n",
      "Accuracy=0.6695234113712375.....Precision=0.6072103393545462.....Recall=0.7479054179854776....F1 score=0.6702540357902641\n",
      "Epoch: 3/50... Step: 4254... Tr Loss: -0.515533... Discr Loss: 0.863818...\n",
      "Accuracy=0.6777591973244147.....Precision=0.649340421342784.....Recall=0.6140383541239992....F1 score=0.6311961722488039\n",
      "Epoch: 4/50... Step: 5672... Tr Loss: -0.566853... Discr Loss: 0.863818...\n",
      "Accuracy=0.6634197324414716.....Precision=0.6038755500656219.....Recall=0.7281698007819772....F1 score=0.6602236758809875\n",
      "Epoch: 5/50... Step: 7090... Tr Loss: -0.598152... Discr Loss: 0.863818...\n",
      "Accuracy=0.6574414715719064.....Precision=0.5876927312775331.....Recall=0.7948240551107801....F1 score=0.6757419865453107\n",
      "Epoch: 6/50... Step: 8508... Tr Loss: -0.622620... Discr Loss: 0.863818...\n",
      "Accuracy=0.6680183946488294.....Precision=0.6154480257192317.....Recall=0.6950288586855334....F1 score=0.6528221046648888\n",
      "Epoch: 7/50... Step: 9926... Tr Loss: -0.639708... Discr Loss: 0.863818...\n",
      "Accuracy=0.664799331103679.....Precision=0.6011586452762924.....Recall=0.7534909700242041....F1 score=0.6687598116169544\n",
      "Epoch: 8/50... Step: 11344... Tr Loss: -0.656725... Discr Loss: 0.863818...\n"
     ]
    }
   ],
   "source": [
    "plot_train=[];plot_val=[]\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "c1= nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "optimizerD = optim.Adam(cnn.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "train_on_gpu=True\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 50 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.to(device)\n",
    "    cnn.to(device)\n",
    "\n",
    "net.train()\n",
    "m = torch.nn.Sigmoid()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    #h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    train_loss=[];discr_err=[]\n",
    "    h=None\n",
    "    for inputs, labels, c_labels in train_loader:\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        #h = tuple([each.data for each in h])\n",
    "        \n",
    "        bs= inputs.shape[0]\n",
    "        inputs=inputs.view(4000, bs, 1)\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        #if(train_on_gpu):\n",
    "        inputs, labels, c_labels = inputs.to(device), labels.to(device), c_labels.to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnn(inputs.view(bs ,1, 4000))\n",
    "        out=m(output)\n",
    "        err = c1(out.squeeze(), c_labels)\n",
    "        err.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())-err.item()\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item()); discr_err.append(err.item())\n",
    "\n",
    "    pred=np.array([]); true_l=np.array([])\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        pred=np.append(pred,predict(net, inputs, inputs.shape[0]), axis=0)\n",
    "        true_l=np.append(true_l,labels.detach().cpu().numpy(), axis=0)\n",
    "    test_acc= accuracy(pred, true_l)\n",
    "    test_prec= precision(pred, true_l)\n",
    "    test_recall= recall(pred, true_l)\n",
    "    test_f1= 2*test_prec*test_recall/(test_prec+test_recall)\n",
    "    print(\"Accuracy=\"+ str(test_acc)+\".....\"+\"Precision=\"+ str(test_prec)+'.....'+\"Recall=\"+ str(test_recall)+'....'+\"F1 score=\"+ str(test_f1))\n",
    "    net.train()\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Tr Loss: {:.6f}...\".format(np.mean(train_loss)), \"Discr Loss: {:.6f}...\".format(np.mean(discr_err)))\n",
    "\n",
    "    torch.save(net.state_dict(), \"adv_gru\"+ str(e))\n",
    "    torch.save(cnn, \"adv_g_cnn\")\n",
    "    plot_train.append(np.mean(train_loss))\n",
    "    plot_val.append(test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.714524466555249,\n",
       " 0.645578483463075,\n",
       " 0.6702540357902641,\n",
       " 0.6311961722488039,\n",
       " 0.6602236758809875,\n",
       " 0.6757419865453107,\n",
       " 0.6528221046648888,\n",
       " 0.6687598116169544,\n",
       " 0.7076874028613558,\n",
       " 0.6686468070236743,\n",
       " 0.6625011157725609,\n",
       " 0.6652069380044652,\n",
       " 0.6569837380262865,\n",
       " 0.6784870991284936,\n",
       " 0.6876859012492564,\n",
       " 0.6835806213624068,\n",
       " 0.6863225479428909,\n",
       " 0.6744356762708754,\n",
       " 0.6674079565109958,\n",
       " 0.6954346466541589,\n",
       " 0.6858473881723677,\n",
       " 0.6974807765835422,\n",
       " 0.6746338707574638,\n",
       " 0.6921688011757295,\n",
       " 0.6630296566717258,\n",
       " 0.6866319800073742,\n",
       " 0.667038127835708,\n",
       " 0.6962398623740478,\n",
       " 0.6823637272878814,\n",
       " 0.6793666920667175,\n",
       " 0.6894078330684272,\n",
       " 0.6832708660782654,\n",
       " 0.6675720409115987,\n",
       " 0.6903998376293891,\n",
       " 0.6783381470784269,\n",
       " 0.6898220768162058,\n",
       " 0.681798060284116,\n",
       " 0.6828682619855925,\n",
       " 0.6751368746604254,\n",
       " 0.678484504299854,\n",
       " 0.6931644278183983,\n",
       " 0.6788389134043157,\n",
       " 0.6903231193120147,\n",
       " 0.6787909404731834,\n",
       " 0.6688417618270799,\n",
       " 0.6809385474860336,\n",
       " 0.6702547247329498,\n",
       " 0.6970073779325209,\n",
       " 0.6831874789633119]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
