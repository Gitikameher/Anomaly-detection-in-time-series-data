{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "from wfdb import rdrecord\n",
    "from wfdb import processing\n",
    "aftdb_path = \"/aii_data/ecg_waveforms/physionet_original_data/aftdb/learning-set/\"\n",
    "test_a= \"/aii_data/ecg_waveforms/physionet_original_data/aftdb/test-set-a/\"\n",
    "test_b=\"/aii_data/ecg_waveforms/physionet_original_data/aftdb/test-set-b/\"\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=None\n",
    "for file in ['04015.npz', '04043.npz', '04048.npz', '04126.npz', '04746.npz', '04908.npz', '04936.npz', '05121.npz', '05261.npz',\n",
    " '06426.npz', '06453.npz', '06995.npz', '07162.npz', '07879.npz', '07910.npz', '08215.npz', '08434.npz', '08455.npz']:\n",
    "    info=np.load('/aii/tianq/data/mitafdb16/'+file)\n",
    "    if type(data)==type(None):\n",
    "        data=info['data']\n",
    "        labels=info['label']\n",
    "    else:\n",
    "        data=np.append(data, info['data'], axis=0)\n",
    "        labels=np.append(labels,info['label'] , axis=0)\n",
    "x= np.append(data[:, :,0], data[:,:,1], axis=0)\n",
    "y= np.append(labels, labels, axis=0)\n",
    "\n",
    "info=np.load('/aii/tianq/data/mitafdb16/Challenge_2017.npz') \n",
    "x= np.append(x, info['data'], axis=0)\n",
    "y= np.append(y, info['label'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=None\n",
    "for file in ['08405.npz', '08378.npz', '07859.npz', '05091.npz', '08219.npz']:\n",
    "    info=np.load('/aii/tianq/data/mitafdb16/'+file)\n",
    "    if type(data)==type(None):\n",
    "        data=info['data']\n",
    "        labels=info['label']\n",
    "    else:\n",
    "        data=np.append(data, info['data'], axis=0)\n",
    "        labels=np.append(labels,info['label'] , axis=0)\n",
    "            \n",
    "test_x= np.append(data[:, :,0], data[:,:,1], axis=0)\n",
    "test_y= np.append(labels, labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=None\n",
    "for file in [\"n01\", \"n02\" , \"n03\" , \"n04\" , \"n05\", \"n06\", \"n07\", \"n08\", \"n09\", \"n10\",\"s01\", \"s02\" , \"s03\" , \"s04\" , \"s05\", \"s06\", \"s07\", \"s08\", \"s09\", \"s10\", \"t01\", \"t02\" , \"t03\" , \"t04\" , \"t05\", \"t06\", \"t07\", \"t08\", \"t09\", \"t10\"]:\n",
    "    record = rdrecord(aftdb_path+file)\n",
    "    if type(data)==type(None):\n",
    "        data=processing.resample_sig(record.p_signal[:,0].ravel(), 128, 250)\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,1].ravel(), 128, 250), axis=0)\n",
    "    else:\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,0].ravel(), 128, 250), axis=0)\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,1].ravel(), 128, 250), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=None\n",
    "for i in range(data.shape[0]):\n",
    "    trial=np.split(data[i, :], [4000, 8000, 12000, 15000])\n",
    "    trial[0]=trial[0].reshape((4000, 1))\n",
    "    trial[1]=trial[1].reshape((4000, 1))\n",
    "    trial[2]=trial[2].reshape((4000, 1))\n",
    "    trial[3]=np.append(trial[3], np.array([0]*1000), axis=0).reshape((4000, 1))\n",
    "    if type(data1)==type(None):\n",
    "        data1= np.concatenate((trial[0], trial[2], trial[3], trial[1]), axis=-1)\n",
    "    else:\n",
    "        data1=np.concatenate((data1, trial[0], trial[2], trial[3], trial[1]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.append(x, data1.T, axis=0)\n",
    "y= np.append(y, np.array([1]*480), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=None\n",
    "for i in range(1, 31):\n",
    "    if i<10:\n",
    "        record = rdrecord(test_a+'a'+str(0)+str(i))\n",
    "    else:\n",
    "        record = rdrecord(test_a+'a'+str(i))\n",
    "    if type(data)==type(None):\n",
    "        data=processing.resample_sig(record.p_signal[:,0].ravel(), 128, 250)\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,1].ravel(), 128, 250), axis=0)\n",
    "    else:\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,0].ravel(), 128, 250), axis=0)\n",
    "        data=np.append(data, processing.resample_sig(record.p_signal[:,1].ravel(), 128, 250), axis=0) \n",
    "for i in range(1,21):\n",
    "    if i<10:\n",
    "        record = rdrecord(test_b+'b'+str(0)+str(i))\n",
    "    else:\n",
    "        record = rdrecord(test_b+'b'+str(i))\n",
    "    data=np.append(data, processing.resample_sig(record.p_signal[:,0].ravel(), 128, 250), axis=0)\n",
    "    data=np.append(data, processing.resample_sig(record.p_signal[:,1].ravel(), 128, 250), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=None\n",
    "for i in range(data.shape[0]):\n",
    "    trial=np.split(data[i, :], [4000, 8000, 12000, 15000])\n",
    "    trial[0]=trial[0].reshape((4000, 1))\n",
    "    trial[1]=trial[1].reshape((4000, 1))\n",
    "    trial[2]=trial[2].reshape((4000, 1))\n",
    "    trial[3]=np.append(trial[3], np.array([0]*1000), axis=0).reshape((4000, 1))\n",
    "    if type(data1)==type(None):\n",
    "        data1= np.concatenate((trial[0], trial[2], trial[3], trial[1]), axis=-1)\n",
    "    else:\n",
    "        data1=np.concatenate((data1, trial[0], trial[2], trial[3], trial[1]), axis=-1)\n",
    "test_data=data1.T\n",
    "test_labels=np.array([1]*800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= np.append(test_data, test_x, axis=0)\n",
    "test_labels= np.append(test_labels, test_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( 'training_data',x)\n",
    "np.save( 'training_labels', y)\n",
    "np.save( 'test_data',test_data)\n",
    "np.save( 'test_labels',test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, freqz, filtfilt, detrend\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=1):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)        \n",
    "    y = filtfilt(b, a, data) #use filtfilt instead of lfilt to avoid a phase delay induced by the filtering\n",
    "    return y\n",
    "\n",
    "\n",
    "def standardlize(in_seq, mean=0, std=1, axis=1):\n",
    "    in_mean = np.mean(in_seq, axis=axis).reshape(-1,1)\n",
    "    in_std = np.std(in_seq, axis=axis).reshape(-1,1)\n",
    "    out_seq =  mean + (in_seq - in_mean ) * std / in_std\n",
    "    return out_seq\n",
    "\n",
    "x=butter_bandpass_filter(x, 1, 30, 250, order=1)\n",
    "x=standardlize(detrend(x))\n",
    "test_data=butter_bandpass_filter(test_data, 1, 30, 250, order=1)\n",
    "test_data=standardlize(detrend(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.load( 'training_data.npy')\n",
    "y=np.load( 'training_labels.npy')\n",
    "test_data=np.load( 'test_data.npy')\n",
    "test_labels=np.load( 'test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_labels))\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, input_dim, hidden_dim, n_layers, drop_prob=0.15):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.GRU(input_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        hidden=self.init_hidden(batch_size)\n",
    "        #hidden = tuple([each.data for each in hidden])\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentGRU(\n",
      "  (lstm): GRU(1, 256, num_layers=2, dropout=0.15)\n",
      "  (dropout): Dropout(p=0.15)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "output_size = 1\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "ngpu=1\n",
    "#device=\"cpu\"\n",
    "device = torch.device(\"cuda:3\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "net = SentimentGRU(output_size, input_dim, hidden_dim, n_layers)\n",
    "#net.load_state_dict(torch.load(\"gru6\",map_location=\"cuda:2\"))\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-20\n",
    "def accuracy(outputs, labels):\n",
    "    temp=(outputs==labels)\n",
    "    return temp.sum()/ (labels.shape[0])\n",
    "\n",
    "def true_positive(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==2).sum()\n",
    "    \n",
    "def true_negative(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==0).sum()\n",
    "\n",
    "def false_positive(outputs, labels):\n",
    "    temp= labels-outputs\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def false_negative(outputs, labels):\n",
    "    temp=outputs-labels\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_positive(outputs, labels))\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_negative(outputs, labels))\n",
    "\n",
    "def BCR(outputs, labels):\n",
    "    return (precision(outputs, labels)+recall(outputs, labels))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_seq, sequence_length):\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    #feature_tensor = torch.from_numpy(test_seq)\n",
    "    feature_tensor=test_seq.view(4000,sequence_length, 1)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    \n",
    "    # get the output from the model\n",
    "    feature_tensor = feature_tensor.type(torch.FloatTensor)\n",
    "    feature_tensor = feature_tensor.to(device)\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "   # print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "  #  if(pred.item()==1):\n",
    "  #      print(\"AFIB wave\")\n",
    " #   else:\n",
    "   #     print(\"Normal wave\")\n",
    "    \n",
    "    return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.6686036789297659.....Precision=0.6472434355058061.....Recall=0.5759635077266804....F1 score=0.6095266243042214\n",
      "Epoch: 1/50... Step: 1418... Loss: 0.569514...\n",
      "Accuracy=0.6025501672240803.....Precision=0.5403199477636305.....Recall=0.7703407186743623....F1 score=0.6351460260198795\n",
      "Epoch: 2/50... Step: 2836... Loss: 0.418934...\n",
      "Accuracy=0.6650501672240803.....Precision=0.5978915662650602.....Recall=0.7761124557810464....F1 score=0.6754435712549623\n",
      "Epoch: 3/50... Step: 4254... Loss: 0.341608...\n",
      "Accuracy=0.6699414715719063.....Precision=0.6093906093906094.....Recall=0.738223794451685....F1 score=0.6676489160176805\n",
      "Epoch: 4/50... Step: 5672... Loss: 0.294970...\n",
      "Accuracy=0.6705685618729097.....Precision=0.6056244464127547.....Recall=0.763824241295848....F1 score=0.6755866611774393\n",
      "Epoch: 5/50... Step: 7090... Loss: 0.266217...\n",
      "Accuracy=0.6905100334448161.....Precision=0.6194462330972311.....Recall=0.8059951591882331....F1 score=0.7005137748290787\n",
      "Epoch: 6/50... Step: 8508... Loss: 0.239753...\n",
      "Accuracy=0.6880434782608695.....Precision=0.6088254810882548.....Recall=0.8541239992552597....F1 score=0.7109096544242989\n",
      "Epoch: 7/50... Step: 9926... Loss: 0.219921...\n",
      "Accuracy=0.7255852842809365.....Precision=0.6881981981981982.....Recall=0.7111338670638615....F1 score=0.6994780697738303\n",
      "Epoch: 8/50... Step: 11344... Loss: 0.204403...\n",
      "Accuracy=0.6852842809364549.....Precision=0.6218347232752085.....Recall=0.7635449636939118....F1 score=0.6854420859100786\n",
      "Epoch: 9/50... Step: 12762... Loss: 0.191033...\n",
      "Accuracy=0.6983695652173914.....Precision=0.6372907746204749.....Recall=0.7620554831502514....F1 score=0.694111162928732\n",
      "Epoch: 10/50... Step: 14180... Loss: 0.176006...\n",
      "Accuracy=0.6940635451505017.....Precision=0.6294615849969752.....Recall=0.7749022528393222....F1 score=0.6946507552365851\n",
      "Epoch: 11/50... Step: 15598... Loss: 0.165998...\n",
      "Accuracy=0.7043478260869566.....Precision=0.6524592888002658.....Recall=0.7310556693353193....F1 score=0.6895249802440951\n",
      "Epoch: 12/50... Step: 17016... Loss: 0.154548...\n",
      "Accuracy=0.7017976588628763.....Precision=0.647970479704797.....Recall=0.7356172035002793....F1 score=0.6890177442560055\n",
      "Epoch: 13/50... Step: 18434... Loss: 0.144562...\n",
      "Accuracy=0.6983277591973244.....Precision=0.6356362517310356.....Recall=0.7691305157326382....F1 score=0.6960404380791911\n",
      "Epoch: 14/50... Step: 19852... Loss: 0.139224...\n",
      "Accuracy=0.7083612040133779.....Precision=0.6480578707343922.....Recall=0.7672686650530628....F1 score=0.7026427962489343\n",
      "Epoch: 15/50... Step: 21270... Loss: 0.131393...\n",
      "Accuracy=0.6961538461538461.....Precision=0.6393837265286471.....Recall=0.7417613107428784....F1 score=0.686778141699707\n",
      "Epoch: 16/50... Step: 22688... Loss: 0.127091...\n",
      "Accuracy=0.7216137123745819.....Precision=0.6754619681993984.....Recall=0.731614224539192....F1 score=0.7024176609911964\n",
      "Epoch: 17/50... Step: 24106... Loss: 0.123951...\n",
      "Accuracy=0.6793060200668897.....Precision=0.6006687209073625.....Recall=0.8529137963135357....F1 score=0.7049047893825736\n",
      "Epoch: 18/50... Step: 25524... Loss: 0.118446...\n",
      "Accuracy=0.7230769230769231.....Precision=0.6706447870048069.....Recall=0.7533047849562465....F1 score=0.7095755875131532\n",
      "Epoch: 19/50... Step: 26942... Loss: 0.113660...\n",
      "Accuracy=0.7008361204013378.....Precision=0.6399250819416263.....Recall=0.7633587786259542....F1 score=0.696213278994736\n",
      "Epoch: 20/50... Step: 28360... Loss: 0.112540...\n",
      "Accuracy=0.7108695652173913.....Precision=0.6410143004570249.....Recall=0.8095326754794265....F1 score=0.7154846141188087\n",
      "Epoch: 21/50... Step: 29778... Loss: 0.113192...\n",
      "Accuracy=0.7184782608695652.....Precision=0.6786096256684492.....Recall=0.708806553714392....F1 score=0.6933794736362808\n",
      "Epoch: 22/50... Step: 31196... Loss: 0.108743...\n",
      "Accuracy=0.710953177257525.....Precision=0.6518324607329843.....Recall=0.7649413517035933....F1 score=0.7038718519787562\n",
      "Epoch: 23/50... Step: 32614... Loss: 0.108155...\n",
      "Accuracy=0.6931438127090301.....Precision=0.6263932233615693.....Recall=0.7847700614410724....F1 score=0.6966942148760331\n",
      "Epoch: 24/50... Step: 34032... Loss: 0.108004...\n"
     ]
    }
   ],
   "source": [
    "plot_train=[];plot_val=[]\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_on_gpu=True\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 50 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.to(device)\n",
    "\n",
    "net.train()\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    #h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    train_loss=[]\n",
    "    h=None\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        #h = tuple([each.data for each in h])\n",
    "        \n",
    "        bs= inputs.shape[0]\n",
    "        inputs=inputs.view(4000, bs, 1)\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        #if(train_on_gpu):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        \n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    pred=np.array([]); true_l=np.array([])\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        pred=np.append(pred,predict(net, inputs, inputs.shape[0]), axis=0)\n",
    "        true_l=np.append(true_l,labels.detach().cpu().numpy(), axis=0)\n",
    "    test_acc= accuracy(pred, true_l)\n",
    "    test_prec= precision(pred, true_l)\n",
    "    test_recall= recall(pred, true_l)\n",
    "    test_f1= 2*test_prec*test_recall/(test_prec+test_recall)\n",
    "    print(\"Accuracy=\"+ str(test_acc)+\".....\"+\"Precision=\"+ str(test_prec)+'.....'+\"Recall=\"+ str(test_recall)+'....'+\"F1 score=\"+ str(test_f1))\n",
    "    net.train()\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(np.mean(train_loss)))\n",
    "\n",
    "    torch.save(net.state_dict(), \"lstm\"+ str(e))\n",
    "    plot_train.append(np.mean(train_loss))\n",
    "    plot_val.append(test_f1)\n",
    "np.save('training_g_lstm', plot_train)\n",
    "np.save('Validation_g_lstm', plot_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu=True\n",
    "pred=np.array([]); true_l=np.array([])\n",
    "for test_data, test_label in test_loader:\n",
    "    pred=np.append(pred,predict(net, test_data, test_data.shape[0]), axis=0)\n",
    "    true_l=np.append(true_l,test_label.detach().cpu().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc= accuracy(pred, true_l)\n",
    "test_prec= precision(pred, true_l)\n",
    "test_recall= recall(pred, true_l)\n",
    "test_f1= 2*test_prec*test_recall/(test_prec+test_recall)\n",
    "print(\"Accuracy=\"+ str(test_acc))\n",
    "print(\"Precision=\"+ str(test_prec))\n",
    "print(\"Recall=\"+ str(test_recall))\n",
    "print(\"F1 score=\"+ str(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6095266243042214,\n",
       " 0.6351460260198795,\n",
       " 0.6754435712549623,\n",
       " 0.6676489160176805,\n",
       " 0.6755866611774393,\n",
       " 0.7005137748290787,\n",
       " 0.7109096544242989,\n",
       " 0.6994780697738303,\n",
       " 0.6854420859100786,\n",
       " 0.694111162928732,\n",
       " 0.6946507552365851,\n",
       " 0.6895249802440951,\n",
       " 0.6890177442560055,\n",
       " 0.6960404380791911,\n",
       " 0.7026427962489343,\n",
       " 0.686778141699707,\n",
       " 0.7024176609911964,\n",
       " 0.7049047893825736,\n",
       " 0.7095755875131532,\n",
       " 0.696213278994736,\n",
       " 0.7154846141188087,\n",
       " 0.6933794736362808,\n",
       " 0.7038718519787562,\n",
       " 0.6966942148760331,\n",
       " 0.6962174940898346,\n",
       " 0.6979332273449921,\n",
       " 0.6987087517934003,\n",
       " 0.7010811503781889,\n",
       " 0.6874889556458738,\n",
       " 0.7022357282396353,\n",
       " 0.7095808383233533,\n",
       " 0.6948083454633673,\n",
       " 0.6983822439279642,\n",
       " 0.7097885932837389,\n",
       " 0.7157956301959311,\n",
       " 0.708353648626686,\n",
       " 0.6934448386551783,\n",
       " 0.6994127516778523,\n",
       " 0.7117698488696449,\n",
       " 0.6989070090215129,\n",
       " 0.7040059347181009,\n",
       " 0.700677045150718,\n",
       " 0.7021267943556585,\n",
       " 0.6958736187845305,\n",
       " 0.7040132323496125,\n",
       " 0.6867235184624042,\n",
       " 0.7056119677405874,\n",
       " 0.7034664958770316,\n",
       " 0.7123761733788987,\n",
       " 0.7093421323984327]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
