{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.load( 'training_data.npy')\n",
    "y=np.load( 'training_labels.npy')\n",
    "test_data_x=np.load( 'test_data.npy')\n",
    "test_labels_y=np.load( 'test_labels.npy')\n",
    "class_labels= np.array([0]*82858+[1]*7354+[2]*480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(class_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_data_x), torch.from_numpy(test_labels_y))\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, num_workers=4)\n",
    "ngpu=1\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, input_dim, hidden_dim, n_layers, drop_prob=0.15):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        hidden=self.init_hidden(batch_size)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator (nn.Module ):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self ).__init__()\n",
    "\n",
    "        outdim = 2440\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.leakyReLU = nn.LeakyReLU()\n",
    "        self.conv_1 = nn.Conv1d(1, 3, kernel_size=27, stride=1)\n",
    "        self.conv_2 = nn.Conv1d(3, 10, kernel_size=14, stride=1)\n",
    "        self.conv_3 = nn.Conv1d(10, 10, kernel_size=3, stride=1)\n",
    "        self.conv_4 = nn.Conv1d(10, 10, kernel_size=4, stride=1)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(outdim,30)\n",
    "        self.fc2 = nn.Linear(30,10)\n",
    "        self.fc3 = nn.Linear(10,3)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_4.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x ):\n",
    "        # 1\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_1(x ) ) )\n",
    "        # 3\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_2(x ) ) )\n",
    "        # 5\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_3(x ) ) )\n",
    "        # 7\n",
    "        x = self.maxpool(self.leakyReLU(self.conv_4(x ) ) )\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 9        \n",
    "        x = self.leakyReLU(self.fc1(x ) )\n",
    "        # 10\n",
    "        x = self.leakyReLU(self.fc2(x ) )\n",
    "    \n",
    "        x = self.fc3(x )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (leakyReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (conv_1): Conv1d(1, 3, kernel_size=(27,), stride=(1,))\n",
      "  (conv_2): Conv1d(3, 10, kernel_size=(14,), stride=(1,))\n",
      "  (conv_3): Conv1d(10, 10, kernel_size=(3,), stride=(1,))\n",
      "  (conv_4): Conv1d(10, 10, kernel_size=(4,), stride=(1,))\n",
      "  (fc1): Linear(in_features=2440, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "# Create the Discriminator\n",
    "cnn = Discriminator().to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    cnn = nn.DataParallel(cnn, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "cnn.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, dropout=0.15)\n",
       "  (dropout): Dropout(p=0.15)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "output_size = 1\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(output_size, input_dim, hidden_dim, n_layers)\n",
    "#net.load_state_dict(torch.load(\"lstm_adam\",map_location=\"cuda:2\"))\n",
    "net.load_state_dict(torch.load(\"lstm10\",map_location=\"cpu\"))\n",
    "device = torch.device(\"cuda:2\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "net.to(device)\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-20\n",
    "def accuracy(outputs, labels):\n",
    "    temp=(outputs==labels)\n",
    "    return temp.sum()/ (labels.shape[0])\n",
    "\n",
    "def true_positive(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==2).sum()\n",
    "    \n",
    "def true_negative(outputs, labels):\n",
    "    temp=outputs+labels\n",
    "    return (temp==0).sum()\n",
    "\n",
    "def false_positive(outputs, labels):\n",
    "    temp= labels-outputs\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def false_negative(outputs, labels):\n",
    "    temp=outputs-labels\n",
    "    return (temp<0).sum()\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_positive(outputs, labels))\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    return true_positive(outputs, labels)/(true_positive(outputs, labels)+false_negative(outputs, labels))\n",
    "\n",
    "def BCR(outputs, labels):\n",
    "    return (precision(outputs, labels)+recall(outputs, labels))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_seq, sequence_length):\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    #feature_tensor = torch.from_numpy(test_seq)\n",
    "    feature_tensor=test_seq.view(4000,sequence_length, 1)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    \n",
    "    # get the output from the model\n",
    "    feature_tensor = feature_tensor.type(torch.FloatTensor)\n",
    "    feature_tensor = feature_tensor.to(device)\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "   # print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "  #  if(pred.item()==1):\n",
    "  #      print(\"AFIB wave\")\n",
    " #   else:\n",
    "   #     print(\"Normal wave\")\n",
    "    \n",
    "    return pred.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.5612040133779265.....Precision=0.5632716049382716.....Recall=0.10193632470675852....F1 score=0.1726312470439855\n",
      "Epoch: 1/50... Step: 1418... Tr Loss: 0.026032... Discr Loss: 0.611155...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eerun/Python-3.7.2/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "plot_train=[];plot_val=[]\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "c1= nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "optimizerD = optim.Adam(cnn.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "train_on_gpu=True\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 50 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.to(device)\n",
    "    cnn.to(device)\n",
    "\n",
    "net.train()\n",
    "m = torch.nn.Sigmoid()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    #h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    train_loss=[];discr_err=[]\n",
    "    h=None\n",
    "    for inputs, labels, c_labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        #h = tuple([each.data for each in h])\n",
    "        \n",
    "        bs= inputs.shape[0]\n",
    "        inputs=inputs.view(4000, bs, 1)\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        #if(train_on_gpu):\n",
    "        inputs, labels, c_labels = inputs.to(device), labels.to(device), c_labels.to(device)\n",
    "        cnn.zero_grad()\n",
    "        output = cnn(inputs.view(bs ,1, 4000))\n",
    "        out=m(output)\n",
    "        err = c1(out.squeeze(), c_labels)\n",
    "        err.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())-err.item()\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item()); discr_err.append(err.item())\n",
    "\n",
    "    pred=np.array([]); true_l=np.array([])\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        pred=np.append(pred,predict(net, inputs, inputs.shape[0]), axis=0)\n",
    "        true_l=np.append(true_l,labels.detach().cpu().numpy(), axis=0)\n",
    "    test_acc= accuracy(pred, true_l)\n",
    "    test_prec= precision(pred, true_l)\n",
    "    test_recall= recall(pred, true_l)\n",
    "    test_f1= 2*test_prec*test_recall/(test_prec+test_recall)\n",
    "    print(\"Accuracy=\"+ str(test_acc)+\".....\"+\"Precision=\"+ str(test_prec)+'.....'+\"Recall=\"+ str(test_recall)+'....'+\"F1 score=\"+ str(test_f1))\n",
    "    net.train()\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Tr Loss: {:.6f}...\".format(np.mean(train_loss)), \"Discr Loss: {:.6f}...\".format(np.mean(discr_err)))\n",
    "\n",
    "    torch.save(net.state_dict(), \"adv_lstm\"+ str(e))\n",
    "    torch.save(cnn, \"adv_cnn\")\n",
    "    plot_train.append(np.mean(train_loss))\n",
    "    plot_val.append(test_f1)\n",
    "np.save('training_lstm', plot_train)\n",
    "np.save('Validation_lstm', plot_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1726312470439855,\n",
       " 0.2585225171929306,\n",
       " 0.33956629932456456,\n",
       " 0.42681593000064344,\n",
       " 0.4792587658208684,\n",
       " 0.4267794922847188,\n",
       " 0.44217099581478775,\n",
       " 0.6186476716989157,\n",
       " 0.5030558880977883,\n",
       " 0.6154172185430463,\n",
       " 0.6257464817988264,\n",
       " 0.4791413786494602,\n",
       " 0.5322009907997169,\n",
       " 0.4550561797752809,\n",
       " 0.6849220370228128,\n",
       " 0.5671113968815303,\n",
       " 0.6431806264003432,\n",
       " 0.6438387024104527,\n",
       " 0.6335795267441313,\n",
       " 0.6134673366834171,\n",
       " 0.637060003647638,\n",
       " 0.6492381236928593,\n",
       " 0.6409789828041126,\n",
       " 0.6416429975966791,\n",
       " 0.6699949716256016,\n",
       " 0.6566868548356378,\n",
       " 0.6495770982433312,\n",
       " 0.6599178761637599,\n",
       " 0.6504176035978156,\n",
       " 0.6539438050101557,\n",
       " 0.6284254528564794,\n",
       " 0.6835104225809068,\n",
       " 0.6591675015939521,\n",
       " 0.6626865671641792,\n",
       " 0.6507953922106418,\n",
       " 0.675580997949419,\n",
       " 0.6623456249452572,\n",
       " 0.6476994887752834,\n",
       " 0.6735594277819867,\n",
       " 0.6564930672979371,\n",
       " 0.6912257200267917,\n",
       " 0.6701246621916137,\n",
       " 0.6916267359701129,\n",
       " 0.6672462508150402,\n",
       " 0.6860914900773629,\n",
       " 0.6562339421684497,\n",
       " 0.6746958135698082,\n",
       " 0.6691711446098244,\n",
       " 0.6618737362390474,\n",
       " 0.6894732363203132]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
